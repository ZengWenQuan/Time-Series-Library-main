pyramid_channels: [16, 32, 64]
kernel_sizes: [3, 5, 7]
use_batch_norm: false
use_attention: True
attention_reduction: 8
fc_hidden_dims: [256, 128]
dropout: 0.1