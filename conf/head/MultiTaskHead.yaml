# Config for MultiTaskHead
in_channels: 64
in_len: 512
self_attention:
  heads: 4
conv_pyramid:
  layers:
    - {out_channels: 128, kernel_size: 3, pool_size: 2}
    - {out_channels: 64, kernel_size: 3, pool_size: 2}
ffn_layers: [512, 128 , 32]
