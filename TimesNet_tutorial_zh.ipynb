{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TimesNet 中文教程\n",
    "**环境配置说明：** 本Notebook为`TimesNet`支持的学习任务提供中文教程。\n",
    "\n",
    "`TimesNet` 支持5大类任务，分别为：长期预测、短期预测、数据插补、异常检测、分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 安装Python 3.8。推荐执行如下命令。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在统计文件夹 '/home/DM14/workspace/lamostdr11/spectral' 的子文件夹中的 .gz 文件...\n",
      "------------------------------\n",
      "【总结报告】\n",
      "- 子文件夹 'A': 451712 个 .gz 文件\n",
      "- 子文件夹 'B': 10386 个 .gz 文件\n",
      "- 子文件夹 'C': 2835 个 .gz 文件\n",
      "- 子文件夹 'F_part_1': 99854 个 .gz 文件\n",
      "- 子文件夹 'F_part_10': 99859 个 .gz 文件\n",
      "- 子文件夹 'F_part_11': 99827 个 .gz 文件\n",
      "- 子文件夹 'F_part_12': 100000 个 .gz 文件\n",
      "- 子文件夹 'F_part_13': 99866 个 .gz 文件\n",
      "- 子文件夹 'F_part_14': 99879 个 .gz 文件\n",
      "- 子文件夹 'F_part_15': 99852 个 .gz 文件\n",
      "- 子文件夹 'F_part_16': 99828 个 .gz 文件\n",
      "- 子文件夹 'F_part_17': 99851 个 .gz 文件\n",
      "- 子文件夹 'F_part_18': 99883 个 .gz 文件\n",
      "- 子文件夹 'F_part_19': 99844 个 .gz 文件\n",
      "- 子文件夹 'F_part_2': 99871 个 .gz 文件\n",
      "- 子文件夹 'F_part_20': 95265 个 .gz 文件\n",
      "- 子文件夹 'F_part_3': 99840 个 .gz 文件\n",
      "- 子文件夹 'F_part_4': 99882 个 .gz 文件\n",
      "- 子文件夹 'F_part_5': 100000 个 .gz 文件\n",
      "- 子文件夹 'F_part_6': 100000 个 .gz 文件\n",
      "- 子文件夹 'F_part_7': 100000 个 .gz 文件\n",
      "- 子文件夹 'F_part_8': 100000 个 .gz 文件\n",
      "- 子文件夹 'F_part_9': 100000 个 .gz 文件\n",
      "- 子文件夹 'G_part_1': 100000 个 .gz 文件\n",
      "- 子文件夹 'G_part_10': 100000 个 .gz 文件\n",
      "- 子文件夹 'G_part_11': 100000 个 .gz 文件\n",
      "- 子文件夹 'G_part_12': 100000 个 .gz 文件\n",
      "- 子文件夹 'G_part_13': 100000 个 .gz 文件\n",
      "- 子文件夹 'G_part_14': 100000 个 .gz 文件\n",
      "- 子文件夹 'G_part_15': 100000 个 .gz 文件\n",
      "- 子文件夹 'G_part_16': 100000 个 .gz 文件\n",
      "- 子文件夹 'G_part_17': 100000 个 .gz 文件\n",
      "- 子文件夹 'G_part_18': 100000 个 .gz 文件\n",
      "- 子文件夹 'G_part_19': 100000 个 .gz 文件\n",
      "- 子文件夹 'G_part_2': 100000 个 .gz 文件\n",
      "- 子文件夹 'G_part_20': 100000 个 .gz 文件\n",
      "- 子文件夹 'G_part_21': 100000 个 .gz 文件\n",
      "- 子文件夹 'G_part_22': 100000 个 .gz 文件\n",
      "- 子文件夹 'G_part_23': 100000 个 .gz 文件\n",
      "- 子文件夹 'G_part_24': 100000 个 .gz 文件\n",
      "- 子文件夹 'G_part_25': 100000 个 .gz 文件\n",
      "- 子文件夹 'G_part_26': 100000 个 .gz 文件\n",
      "- 子文件夹 'G_part_27': 100000 个 .gz 文件\n",
      "- 子文件夹 'G_part_28': 100000 个 .gz 文件\n",
      "- 子文件夹 'G_part_29': 100000 个 .gz 文件\n",
      "- 子文件夹 'G_part_3': 100000 个 .gz 文件\n",
      "- 子文件夹 'G_part_30': 100000 个 .gz 文件\n",
      "- 子文件夹 'G_part_31': 100000 个 .gz 文件\n",
      "- 子文件夹 'G_part_32': 100000 个 .gz 文件\n",
      "- 子文件夹 'G_part_33': 4544 个 .gz 文件\n",
      "- 子文件夹 'G_part_4': 100000 个 .gz 文件\n",
      "- 子文件夹 'G_part_5': 100000 个 .gz 文件\n",
      "- 子文件夹 'G_part_6': 100000 个 .gz 文件\n",
      "- 子文件夹 'G_part_7': 100000 个 .gz 文件\n",
      "- 子文件夹 'G_part_8': 100000 个 .gz 文件\n",
      "- 子文件夹 'G_part_9': 100000 个 .gz 文件\n",
      "- 子文件夹 'K_part_1': 100000 个 .gz 文件\n",
      "- 子文件夹 'K_part_10': 47111 个 .gz 文件\n",
      "- 子文件夹 'K_part_2': 100000 个 .gz 文件\n",
      "- 子文件夹 'K_part_3': 100000 个 .gz 文件\n",
      "- 子文件夹 'K_part_4': 100000 个 .gz 文件\n",
      "- 子文件夹 'K_part_5': 100000 个 .gz 文件\n",
      "- 子文件夹 'K_part_6': 100000 个 .gz 文件\n",
      "- 子文件夹 'K_part_7': 100000 个 .gz 文件\n",
      "- 子文件夹 'K_part_8': 60943 个 .gz 文件\n",
      "- 子文件夹 'K_part_9': 100000 个 .gz 文件\n",
      "- 子文件夹 'M': 138420 个 .gz 文件\n",
      "- 子文件夹 'O': 190 个 .gz 文件\n",
      "- 子文件夹 'Other': 24127 个 .gz 文件\n",
      "------------------------------\n",
      "总计: 6733669 个 .gz 文件\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# --- 请在这里修改为您要统计的文件夹路径 ---\n",
    "parent_directory_path = \"/home/DM14/workspace/lamostdr11/spectral\"\n",
    "# -----------------------------------------\n",
    "\n",
    "def count_gz_files_in_subdirs(parent_dir):\n",
    "    \"\"\"\n",
    "    统计指定父文件夹下所有直接子文件夹中包含的 .gz 文件数量。\n",
    "\n",
    "    Args:\n",
    "        parent_dir (str): 要统计的父文件夹路径。\n",
    "    \"\"\"\n",
    "    # 检查路径是否存在且是否为文件夹\n",
    "    if not os.path.isdir(parent_dir):\n",
    "        print(f\"错误：提供的路径 '{parent_dir}' 不是一个有效的文件夹。\")\n",
    "        return\n",
    "\n",
    "    print(f\"正在统计文件夹 '{parent_dir}' 的子文件夹中的 .gz 文件...\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    subdir_gz_counts = {}\n",
    "    total_gz_count = 0\n",
    "\n",
    "    # 遍历父文件夹下的所有项目\n",
    "    for name in sorted(os.listdir(parent_dir)):\n",
    "        full_path = os.path.join(parent_dir, name)\n",
    "        \n",
    "        # 检查是否是文件夹\n",
    "        if os.path.isdir(full_path):\n",
    "            try:\n",
    "                # 统计该子文件夹下的 .gz 文件数量\n",
    "                gz_files = [f for f in os.listdir(full_path) \n",
    "                            if f.endswith('.gz') and os.path.isfile(os.path.join(full_path, f))]\n",
    "                count = len(gz_files)\n",
    "                subdir_gz_counts[name] = count\n",
    "                total_gz_count += count\n",
    "            except OSError as e:\n",
    "                print(f\"无法访问子文件夹 '{name}' 或其内容: {e}\")\n",
    "\n",
    "\n",
    "    # --- 生成并打印总结报告 ---\n",
    "    print(\"【总结报告】\")\n",
    "    if not subdir_gz_counts:\n",
    "        print(\"未在任何子文件夹中找到 .gz 文件或没有子文件夹。\")\n",
    "    else:\n",
    "        for subdir, count in subdir_gz_counts.items():\n",
    "            print(f\"- 子文件夹 '{subdir}': {count} 个 .gz 文件\")\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"总计: {total_gz_count} 个 .gz 文件\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 检查用户是否修改了路径\n",
    "    \n",
    "    count_gz_files_in_subdirs(parent_directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7005030\n",
      "11305056\n",
      "11305149\n",
      "11405232\n",
      "16005090\n",
      "18705085\n",
      "20105180\n",
      "20205063\n",
      "21105191\n",
      "21305164\n",
      "21905066\n",
      "21905074\n",
      "27705085\n",
      "27705180\n",
      "27705185\n",
      "28705232\n",
      "29305022\n",
      "29305215\n",
      "32005048\n",
      "32005149\n",
      "33105193\n",
      "33205056\n",
      "33605183\n",
      "34005023\n",
      "34005143\n",
      "34005173\n",
      "34105060\n",
      "34505024\n",
      "34505218\n",
      "36005051\n",
      "36005085\n",
      "36005213\n",
      "36505172\n",
      "37205166\n",
      "37205232\n",
      "37505138\n",
      "37605166\n",
      "38005030\n",
      "38205102\n",
      "38205120\n",
      "38205124\n",
      "38205197\n",
      "40005217\n",
      "40005228\n",
      "41705166\n",
      "42105037\n",
      "42105138\n",
      "42105143\n",
      "42105144\n",
      "42105168\n",
      "42105197\n",
      "42305182\n",
      "44105081\n",
      "44105187\n",
      "45905019\n",
      "45905080\n",
      "45905114\n",
      "45905140\n",
      "46005065\n",
      "46205114\n",
      "46405004\n",
      "46405030\n",
      "48405154\n",
      "48705041\n",
      "48705081\n",
      "48705238\n",
      "48805158\n",
      "48805191\n",
      "49205080\n",
      "49205144\n",
      "异常obsid已记录到: dataset/ljf_5w/abnormal_obsid.txt\n",
      "异常obsid及feh已记录到: dataset/ljf_5w/abnormal_obsid_with_feh.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "root_dir = 'dataset/ljf_5w'\n",
    "output_file = 'dataset/ljf_5w/abnormal_obsid.txt'\n",
    "abnormal_obsids = set()\n",
    "\n",
    "# 先收集所有异常obsid\n",
    "for subdir, _, files in os.walk(root_dir):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv') or file.endswith('.feather'):\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            try:\n",
    "                if file.endswith('.csv'):\n",
    "                    df = pd.read_csv(file_path)\n",
    "                else:\n",
    "                    df = pd.read_feather(file_path)\n",
    "                nan_rows = df[df.isnull().any(axis=1)]\n",
    "                if not nan_rows.empty:\n",
    "                    obsids = nan_rows.iloc[:, 0].dropna().astype(int).tolist()  # 强制转为整型\n",
    "                    abnormal_obsids.update(obsids)\n",
    "            except Exception as e:\n",
    "                print(f\"读取文件失败: {file_path}, 错误: {e}\")\n",
    "\n",
    "# 写入异常obsid到txt\n",
    "with open(output_file, 'w') as f:\n",
    "    for obsid in sorted(abnormal_obsids):\n",
    "        print(obsid)\n",
    "        f.write(f\"{obsid}\\n\")\n",
    "\n",
    "# 查询所有labels.csv，将异常obsid的FeH值对应输出\n",
    "output_feh_file = 'dataset/ljf_5w/abnormal_obsid_with_feh.txt'\n",
    "with open(output_feh_file, 'w') as f_feh:\n",
    "    for subdir, _, files in os.walk(root_dir):\n",
    "        if 'labels.csv' in files:\n",
    "            label_path = os.path.join(subdir, 'labels.csv')\n",
    "            try:\n",
    "                df_label = pd.read_csv(label_path)\n",
    "                obsid_col = df_label.columns[0]\n",
    "                # obsid列强制转为整型\n",
    "                df_label[obsid_col] = df_label[obsid_col].astype(int)\n",
    "                for obsid in abnormal_obsids:\n",
    "                    match = df_label[df_label[obsid_col] == obsid]\n",
    "                    if not match.empty:\n",
    "                        feh = match.iloc[0].get('FeH', '标签无feh字段')\n",
    "                        f_feh.write(f\"{obsid}\\t{feh}\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"读取labels.csv失败: {label_path}, 错误: {e}\")\n",
    "\n",
    "print(f\"异常obsid已记录到: {output_file}\")\n",
    "print(f\"异常obsid及feh已记录到: {output_feh_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已处理并保存: dataset/ljf_5w/val/feature.feather, dataset/ljf_5w/val/labels.csv\n",
      "已处理并保存: dataset/ljf_5w/train/feature.feather, dataset/ljf_5w/train/labels.csv\n",
      "已处理并保存: dataset/ljf_5w/train/feature.feather, dataset/ljf_5w/train/labels.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "root_dir = 'dataset/ljf_5w'\n",
    "\n",
    "for subdir, _, files in os.walk(root_dir):\n",
    "    if 'feature.feather' in files and 'labels.csv' in files:\n",
    "        feature_path = os.path.join(subdir, 'feature.feather')\n",
    "        label_path = os.path.join(subdir, 'labels.csv')\n",
    "        try:\n",
    "            df_feature = pd.read_feather(feature_path)\n",
    "            df_label = pd.read_csv(label_path)\n",
    "            obsid_col = df_feature.columns[0]\n",
    "            # 对齐\n",
    "            df_merged = pd.merge(df_feature, df_label, on=obsid_col, how='inner')\n",
    "            # 检查每行是否有缺失值或异常值\n",
    "            clean_rows = df_merged[~df_merged.isnull().any(axis=1)]\n",
    "            # 拆分回特征和标签\n",
    "            feature_clean = clean_rows[df_feature.columns]\n",
    "            label_clean = clean_rows[df_label.columns]\n",
    "            # 保存新文件\n",
    "            feature_clean_path = os.path.join(subdir, 'feature.feather')\n",
    "            label_clean_path = os.path.join(subdir, 'labels.csv')\n",
    "            feature_clean.reset_index(drop=True).to_feather(feature_clean_path)\n",
    "            label_clean.reset_index(drop=True).to_csv(label_clean_path, index=False)\n",
    "            print(f\"已处理并保存: {feature_clean_path}, {label_clean_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"处理失败: {subdir}, 错误: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 导入依赖包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.fft\n",
    "\n",
    "from layers.Embed import DataEmbedding\n",
    "from layers.Conv_Blocks import Inception_Block_V1   # 用于2D时序数据卷积的模块，可更换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. TimesBlock 构建\n",
    "`TimesNet`的核心思想在于`TimesBlock`的构建。其主要通过对数据进行FFT获取基频，然后将时间序列根据主基频分别重塑为2D变化，接着进行2D卷积，最后加权回原序列得到输出。\n",
    "\n",
    "下面详细介绍`TimesBlock`的实现。\n",
    "\n",
    "TimesBlock包含两个主要成员。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimesBlock(nn.Module):\n",
    "    def __init__(self, configs):\n",
    "        ...\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先关注`__init__(self, configs)`的实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, configs):    # configs为TimesBlock的配置\n",
    "    super(TimesBlock, self).__init__()\n",
    "    self.seq_len = configs.seq_len   # 序列长度\n",
    "    self.pred_len = configs.pred_len # 预测长度\n",
    "    self.k = configs.top_k    # 选取的主频数量\n",
    "    # 参数高效设计\n",
    "    self.conv = nn.Sequential(\n",
    "        Inception_Block_V1(configs.d_model, configs.d_ff, num_kernels=configs.num_kernels),\n",
    "        nn.GELU(),\n",
    "        Inception_Block_V1(configs.d_ff, configs.d_model, num_kernels=configs.num_kernels)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，关注`forward(self, x)`的实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "    B, T, N = x.size()  # B:批大小 T:序列长度 N:特征数\n",
    "    period_list, period_weight = FFT_for_Period(x, self.k)\n",
    "    res = []\n",
    "    for i in range(self.k):\n",
    "        period = period_list[i]\n",
    "        if (self.seq_len + self.pred_len) % period != 0:\n",
    "            length = (((self.seq_len + self.pred_len) // period) + 1) * period\n",
    "            padding = torch.zeros([x.shape[0], (length - (self.seq_len + self.pred_len)), x.shape[2]]).to(x.device)\n",
    "            out = torch.cat([x, padding], dim=1)\n",
    "        else:\n",
    "            length = (self.seq_len + self.pred_len)\n",
    "            out = x\n",
    "        out = out.reshape(B, length // period, period, N).permute(0, 3, 1, 2).contiguous()\n",
    "        out = self.conv(out)\n",
    "        out = out.permute(0, 2, 3, 1).reshape(B, -1, N)\n",
    "        res.append(out[:, :(self.seq_len + self.pred_len), :])\n",
    "    res = torch.stack(res, dim=-1)\n",
    "    period_weight = F.softmax(period_weight, dim=1)\n",
    "    period_weight = period_weight.unsqueeze(1).unsqueeze(1).repeat(1, T, N, 1)\n",
    "    res = torch.sum(res * period_weight, -1)\n",
    "    res = res + x\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述`FFT_for_Period`函数定义如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FFT_for_Period(x, k=2):\n",
    "    xf = torch.fft.rfft(x, dim=1)\n",
    "    frequency_list = abs(xf).mean(0).mean(-1)\n",
    "    frequency_list[0] = 0\n",
    "    _, top_list = torch.topk(frequency_list, k)\n",
    "    top_list = top_list.detach().cpu().numpy()\n",
    "    period = x.shape[1] // top_list\n",
    "    return period, abs(xf).mean(-1)[:, top_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更直观的理解可参考下图：\n",
    "\n",
    "![FFT 示意图](./tutorial/fft.png)\n",
    "\n",
    "![2D 卷积示意图](./tutorial/conv.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更多细节可参考我们的论文：\n",
    "(链接: https://openreview.net/pdf?id=ju_Uqw384Oq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. TimesNet整体结构\n",
    "\n",
    "有了`TimesBlock`，我们可以构建`TimesNet`，它擅长提取时序数据的周期性信息，支持多种任务。\n",
    "\n",
    "下面介绍`TimesNet`的整体结构和多任务能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, configs):\n",
    "        ...\n",
    "    def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n",
    "        ...\n",
    "    def imputation(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask):\n",
    "        ...\n",
    "    def anomaly_detection(self, x_enc):\n",
    "        ...\n",
    "    def classification(self, x_enc, x_mark_enc):\n",
    "        ...\n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先关注`__init__(self, configs)`的实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, configs):\n",
    "    super(Model, self).__init__()\n",
    "    self.configs = configs\n",
    "    self.task_name = configs.task_name\n",
    "    self.seq_len = configs.seq_len\n",
    "    self.label_len = configs.label_len\n",
    "    self.pred_len = configs.pred_len\n",
    "    self.model = nn.ModuleList([TimesBlock(configs) for _ in range(configs.e_layers)])\n",
    "    self.enc_embedding = DataEmbedding(configs.enc_in, configs.d_model, configs.embed, configs.freq, configs.dropout)\n",
    "    self.layer = configs.e_layers\n",
    "    self.layer_norm = nn.LayerNorm(configs.d_model)\n",
    "    if self.task_name == 'long_term_forecast' or self.task_name == 'short_term_forecast':\n",
    "        self.predict_linear = nn.Linear(self.seq_len, self.pred_len + self.seq_len)\n",
    "        self.projection = nn.Linear(configs.d_model, configs.c_out, bias=True)\n",
    "    if self.task_name == 'imputation' or self.task_name == 'anomaly_detection':\n",
    "        self.projection = nn.Linear(configs.d_model, configs.c_out, bias=True)\n",
    "    if self.task_name == 'classification':\n",
    "        self.act = F.gelu\n",
    "        self.dropout = nn.Dropout(configs.dropout)\n",
    "        self.projection = nn.Linear(configs.d_model * configs.seq_len, configs.num_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 预测任务\n",
    "预测的基本思想是将已知序列扩展到(seq_len+pred_len)长度，通过多层TimesBlock和归一化提取周期信息，最后投影到输出空间。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
